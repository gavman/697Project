\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{18-697}{Fall '14 Pittsburgh, PA USA}
\CopyrightYear{2014}
% --- End of Author Metadata ---

\title{Clustering for Controlling Stochastic Optimization using Feedback}
\subtitle{Applied to Newspaper Article Title Category Prediction}

\numberofauthors{2} 
\author{
\alignauthor
Kevin Brennan\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{5000 Forbes Avenue}\\
       \affaddr{Pittsburgh, PA}\\
       \email{kbrennan@andrew.cmu.edu}
% 2nd. author
\alignauthor
Gavriel Adler\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{5000 Forbes Avenue}\\
       \affaddr{Pittsburgh, PA}\\
       \email{gya@andrew.cmu.edu}\\
}


\maketitle
\begin{abstract}
In this paper, we develop a method that uses a feedback controller in order to control the desired number
of local optima.
\end{abstract}

%%@Kevin-->Not sure what these are
% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous} %I commented out this line
%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures] %I commented out this line

\terms{Classification, Feedback, Stochastic Optimization} %I put some down...

\keywords{Controlled Clustering, Naive Bayes, KMeans, Title Category Classification} %I put some down...

\section{Introduction}
In determining optimal feature sets for classification, there are often multiple local optima. In order to reduce the computational complexity of dealing with a variable number of optima it desirable to refine the set of optima to a given size. This paper details the use of generalized crowding and a feedback controller to cluster a set of features

\section{Related Research}
Background

\section{Method}
\begin{figure*}[t]
\centering
\fbox{
  \includegraphics[width=0.8\textwidth]{pipeline.png}}
\caption{The Algorithm Pipeline}
\label{fig:pipeline}
\end{figure*}
\subsection{Pipeline Overview}
Our pipeline, as seen in Figure~\ref{fig:pipeline}, consists of a few independent pieces put together in a control loop. To initialize the pipeline, we first separated the titles into a training and a testing set. We then made a list of words that appear in the titles of the training set and filtered the data set into our feature sets, or sets of bags of words from that list. In the control loop, we get our error value from the difference between the optimal number of clusters returned by the objective K-Means function and the desired number of clusters. The error and the PID controller created a $\phi$ term representing exploration vs. exploitation. We generated sets of bags of words from the training set list and passed each into the genetic algorithm along with the $\phi$ term, and each returned a locally optimal bag of words. These optimal bags were clustered to find the optimal number of clusters, and the loop restarts. Each section of the algorithm is discussed in detail below.

\subsection{Genetic Algorithm}
\begin{figure}[t]
\centering
\fbox{\rule{0pt}{2in}
  \includegraphics[width=0.5\textwidth]{genetic_algo.png}}
\caption{The flow of one iteration of the genetic algorithm}
\label{fig:genetic_algo}
\end{figure}
Our genetic algorithm operated on a set of bags of words, and converged to a single locally optimal bag of words. At each round of the genetic algorithm, the Naive Bayes score on the testing set was calculated for each bag of words. The lowest two scores competed for survival, and the victor was based on both the $\phi$ exploration/exploitation term and a random coin flip. After one bag was killed, each remaining bag was mutated and additional randomized cross-over between trials was added in to further diversity. Our solution set was created by running several trials of the genetic algorithm on many randomly generated sets of bags of words.\\
\indent In the mutation step, the algorithm attempted to replace each word in the bag of words with a new word taken from the titles in the training set. If the Naive Bayes score of the bag of words with the new word was higher than it was with the old word, the new word replaced the old word, otherwise the new word was discarded.\\
\indent In our genetic algorithm, we did not follow CITEPAPERHERE and user Hamiltonian Distance in the clustering algorithm, instead we decided to try something different and creative and used Naive Bayes results. We passed the number of true positives, true negatives, false positives, and false negatives to the clustering algorithm, hoping this would create diverse results with some bags of words strong in each case. In the end we found this approach did not fare well and the results converged to a single cluster very quickly, and therefore would recommend not using our process but Hamiltonian Distance in the future.

\subsection{Clustering Algorithm}
Our clustering algorithm is based on the Objective K Nearest Neighbor function. The objective function works by running K-Means with a variable number of nodes used for clustering. For each element of the solution set created by the genetic algorithm trials, we calculate the number of true positives, false positives, true negatives, and false negative. A true positive or true negative represents the correct classification of a title, and conversely a false positive or false negative represents a failure to do so. This representation is possible because we are separating the titles into only two classes. Based on these statistics, we calculate a distance to each node based on the Euclidean distance metric. For K-Means, we group each element to the closest node, and re-calculate a mean node for each grouping. K-Means continues to iterate until the mean nodes remain constant between iterations. Once K-Means finishes iterating, we calculate the distance of each element to its group's mean node. The sum of these distances is our error for that given number of nodes.
Based on the error for each number of nodes, we find the change in the error with respect to the change in the number of nodes. This change is the first derivative for our error. Next, we find the change in the first derivative with respect to the change in the number of nodes. This change is the second derivative for our error. Based on the maximum second derivative, we calculate the elbow point for the error versus number of nodes. For our feedback loop, the elbow point returned by our clustering algorithm represents the optimal number of nodes, or local optima, to use.

\subsection{Feedback Controller}
For controlling the exploration/exploitation term for the genetic algorithm, we use a PID controller. The set point for the controller is the desired number of clusters after crowding. The error is the difference between the set point and the returned number of clusters from the clustering algorithm. Based on this error, we calculate the change in the exploration/exploitation term for the next iteration of the feedback loop. The change is determined by calculating the change in error between iterations of the feedback loop, which is our derivative term, and the sum of all past errors, which is our integral term. By multiplying these terms and the error by set gains, we calculate the new exploration/exploitation term. The goal of implementing a full PID controller is to improve upon the efficiency of the entire pipeline by reducing the number of iterations till convergence to the desired number of clusters. The derivative term should improve the responsiveness of the controller while the integral term should ensure that the limit of the error goes to zero in time.

\subsection{Implementation}

\section{Results}

\section{Conclusions}
\cite{ole:feedback}

\section{Future Work}
As mentioned, in the future we would like to 

\section{Acknowledgments}

\bibliography{sigproc}
\bibliographystyle{plain}

\end{document}
