
\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{18-697}{Fall '14 Pittsburgh, PA USA}
\CopyrightYear{2014}
% --- End of Author Metadata ---

\title{Clustering for Controlling Stochastic Optimization using Feedback}
\subtitle{Applied to Newspaper Article Title Category Prediction}

\numberofauthors{2} 
\author{
\alignauthor
Kevin Brennan\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{5000 Forbes Avenue}\\
       \affaddr{Pittsburgh, PA}\\
       \email{kbrennan@andrew.cmu.edu}
% 2nd. author
\alignauthor
Gavriel Adler\\
       \affaddr{Carnegie Mellon University}\\
       \affaddr{5000 Forbes Avenue}\\
       \affaddr{Pittsburgh, PA}\\
       \email{gya@andrew.cmu.edu}\\
}


\maketitle
\begin{abstract}
In optimizing feature sets for classification, there are often multiple local optima when using tools such as genetic algorithms. In order to reduce complexity and computational cost, it is often desirable to deal with only a fixed number of solutions. One method for refining the solution set is niching through generalized crowding.\\
\indent In this paper, we explore using a genetic algorithm for creating optimized feature sets for classification problems. Through several trials of a genetic algorithm, we create a solution space of locally optimal feature sets. Using generalized crowding with K-Means clustering, we refine the solution space into a set of niches. Based on the difference between a desired number of niches and the returned value, we implement a feedback controller for the exploration/exploitation of the genetic algorithm. By tuning this parameter, we adjust the range of the solution space through several trials till it converges to a desired number of niches. After testing on classifying newspaper titles, the algorithm did not show the controller to be effective in converging the solution space to a desired number of niches. Control over the exploration/exploitation parameter did not have sufficient power to augment the number of niches.
\end{abstract}

\terms{Classification, Feedback, Stochastic Optimization}

\keywords{Controlled Clustering, Naive Bayes, K-Means, Title Category Classification}

\section{Introduction}
The problem of optimizing feature sets for use in classification is often multi-modal. Often there are variable number of local optima each with unique characteristics. Although multiple solutions often give a better representation of the solution space for a given optimization, having a variable number of solutions can drastically increase the complexity and cost of a given algorithm.\\
\indent One method for creating a fixed size solution set is through generalized crowding. Crowding groups solutions based on a given fitness function into a set of niches\cite{ole:feedback}. Using a scaling factor, generalized crowding controls the range of solutions  with respect to the given fitness function. In the case of evolutionary algorithms such as genetic algorithms, the scaling factor controls the exploration/exploitation parameter used in determining survival between generations. Control of the scaling factor therefore also controls the number of niches within a given solution space\cite{ole:feedback}.\\
\indent In this paper, we detail using genetic algorithm to optimize feature sets for classifying newspaper titles into topics. For classification, each feature set is a set of bit-strings. Through several trials of a genetic algorithm, we create a set of locally optimized feature sets. With K-Means, we implement generalized crowding in order to cluster the solution set into a set of niches. We apply a feedback controller on the error between the desired number of niches and this returned number. In this paper, we consider the use a full PID controller in order to expand upon past research in this area. Using derivative and integral gains allows for greater control of the rate of convergence, overshoot, and steady state error when refining the number of  niches. Further information on the implementation of our algorithm is provided in the Methods section of our paper.  

\section{Background}
\subsection{Feature Selection}
\cite{feature:feedback}

\subsection{Generalized Crowding}

\subsection{Feedback Controller}

\section{Algorithm}
\begin{figure*}[t]
\centering
\fbox{
  \includegraphics[width=0.8\textwidth]{pipeline.png}}
\caption{The Algorithm Pipeline}
\label{fig:pipeline}
\end{figure*}
\subsection{Pipeline Overview}
Our pipeline, as seen in Figure~\ref{fig:pipeline}, consists of a few independent pieces put together in a control loop. To initialize the pipeline, we first separated the titles into a training and a testing set. We then made a list of words that appear in the titles of the training set and filtered the data set into our feature sets, or sets of bags of words from that list. In the control loop, we get our error value from the difference between the optimal number of clusters returned by the objective K-Means function and the desired number of clusters. The error and the PID controller created a $\phi$ term representing exploration vs. exploitation. We generated sets of bags of words from the training set list and passed each into the genetic algorithm along with the $\phi$ term, and each returned a locally optimal bag of words. These optimal bags were clustered to find the optimal number of clusters, and the loop restarts. Each section of the algorithm is discussed in detail below.

\subsection{Genetic Algorithm}
\begin{figure}[t]
\centering
\fbox{\rule{0pt}{2in}
  \includegraphics[width=0.45\textwidth]{genetic_algo.png}}
\caption{The flow of one iteration of the genetic algorithm}
\label{fig:genetic_algo}
\end{figure}
Our genetic algorithm operated on a set of bags of words, and converged to a single locally optimal bag of words. At each round of the genetic algorithm, the Naive Bayes score on the testing set was calculated for each bag of words. The lowest two scores competed for survival, and the victor was based on both the $\phi$ exploration/exploitation term and a random coin flip. After one bag was killed, each remaining bag was mutated and additional randomized cross-over between trials was added in to further diversity. Our solution set was created by running several trials of the genetic algorithm on many randomly generated sets of bags of words.\\
\indent In the mutation step, the algorithm attempted to replace each word in the bag of words with a new word taken from the titles in the training set. If the Naive Bayes score of the bag of words with the new word was higher than it was with the old word, the new word replaced the old word, otherwise the new word was discarded.\\
\indent This process of testing with Naive Bayes continues till only a single bag of words is left. We add this bag of words to our solution set, and after all the trials of the genetic algorithm are finished we have a set locally optimal solutions. This solution set is then passed onto the clustering algorithm.

\subsection{Clustering Algorithm}
Our clustering algorithm is based on the Objective K-Means function. The objective function works by running K-Means with a variable number of nodes used for clustering. For each element of the solution set created by the genetic algorithm trials, we calculate the number of true positives, false positives, true negatives, and false negative. A true positive or true negative represents the correct classification of a title, and conversely a false positive or false negative represents a failure to do so. This representation is possible because we are separating the titles into only two classes. We used these statistics, instead of other distance metrics such as the Hamming distance as used in previous research\cite{ole:feedback}, in order to cluster the solutions by characteristics. For instance, to have on group of solutions with minimal false positive or another with minimal false negatives.\\
\indent Based on these statistics, we calculate a distance to each node based on the Euclidean distance metric. For K-Means, we group each element to the closest node, and re-calculate a mean node for each grouping. K-Means continues to iterate until the mean nodes remain constant between iterations. Once K-Means finishes iterating, we calculate the distance of each element to its group's mean node. The sum of these distances is our error for that given number of nodes.\\
\indent Based on the error for each number of nodes, we find the change in the error with respect to the change in the number of nodes. This change is the first derivative for our error. Next, we find the change in the first derivative with respect to the change in the number of nodes. This change is the second derivative for our error. Based on the maximum second derivative, we calculate the elbow point for the error versus number of nodes. For our feedback loop, the elbow point returned by our clustering algorithm represents the optimal number of nodes, or local optima, to use.

\subsection{Feedback Controller}
For controlling $\phi$ for the genetic algorithm, we use a PID controller. The set point for the controller is the desired number of clusters after crowding. The error is the difference between the set point and the returned number of clusters from the clustering algorithm. Based on this error, we calculate the change in the exploration/exploitation term for the next iteration of the feedback loop. The change is determined by calculating the change in error between iterations of the feedback loop, which is our derivative term, and the sum of all past errors, which is our integral term. By multiplying these terms and the error by set gains, we calculate the new exploration/exploitation term. The goal of implementing a full PID controller is to improve upon the efficiency of the entire pipeline by reducing the number of iterations till convergence to the desired number of clusters. The derivative term should improve the responsiveness of the controller while the integral term should ensure that the limit of the error goes to zero in time.

\subsection{Implementation}
We implemented our pipeline using Python's PANDAS module. Python, a scripting language, is easy to work with and develop quickly which was important given the project had to be finished within a semester. PANDAS is built on top of Python's NumPy library, which is strictly typed and invokes C calls, significantly speeding up the slow execution of Python. Lastly, we implemented clustering using Python's scikit-learn library, a machine learning library. Scikit-learn is also built in NumPy, making passing data between the two libraries very simple. In the end we were able to run our pipeline very efficiently, with each iteration of the feedback loop taking on the order of seconds instead of minutes. This speed allowed us to try many different runs of various combinations of parameters and analyze the results.

\section{Experiments}
\subsection{Data}
For our experiments, we used the Boydstun New York Times Front Page Data Set\cite{boydstun}. The data set contains every NYT front page article title from 1/1/1996 through 12/31/2006. The data set used ten different categories for classification. Each title can be marked as one or none of the given categories. In order to limit our algorithm to two topics, we chose "War on Terror" and "Presidential Elections". These two topics were chosen because they are similar enough that classifying titles a either one becomes a significant challenge and different enough that it is possible to differentiate between the two. Based on these two topics, we filtered the data set by removing any titles that did not fit into either of these two topics. Based on the remaining titles, we created a bag of words or bit-strings. With this bag of words, we create randomized sets of words with each set being a fixed size. These sets are the feature sets used in the later classification stages of the algorithm.
 
\subsection{Results}
In testing, we experimented with different controller gains, number of features per feature set, and number of trials of the genetic algorithm. For data, we recorded the clustering error per each number of niches used, the number of niches returned by the objective clustering algorithm, and the $\phi$ term, for each iteration of the control loop.
\begin{figure}[t]
\centering
\fbox{\rule{0pt}{2in}
  \includegraphics[width=0.45\textwidth]{graph2.png}}
\caption{A sample of the clustering error versus number of niches created by the clustering algorithm.}
\label{fig:graph1}
\end{figure}
A sample representation of the objective clustering algorithm is shown in Figure 3. The graph shows the clustering error, or the sum of the Euclidean distances of each element of the solution set to its respective node. Based on this error, we calculate the first and second derivatives of the error with respect to number of niches used for clustering. Our algorithm determines the optimal number of niches to be the point with the maximum second derivative. The result of the iteration displayed in this graph is two niches for clustering. This graph is representative of the majority of the trials of our entire algorithm. Even through varying the $\phi$ term and the number of words per set, overall the number of niches remains at 2. This result is shown in Figure 4.
\begin{figure}[t]
\centering
\fbox{\rule{0pt}{2in}
  \includegraphics[width=0.45\textwidth]{graph1.png}}
\caption{The number of niches chosen by the clustering algorithm across the iterations of the feedback loop.}
\label{fig:graph2}
\end{figure}
Figure 4 shows the output of the control loop after each generation for several different trials. Trial 0 is the baseline with a proportional controller with a gain of 0.01 and five bit-strings per feature set. Trial 1 is with a proportional and derivative controller with gains of 1 and 0.1 and five bit-strings per feature set. Trial 2 is with the same controller as Trial 0, but with 10 bit-strings per feature set. Trial 3 is with the same controller as Trial 0, but with 15 bit-strings per feature set. The graph shows that the returned number of niches is invariant to these parameters. In addition, we noted that the classification accuracy remained constant at approximately seventy percent with a margin of 0.02. This accuracy is approximately equal to the score obtained when using our entire bag of words as a classifier. These results shows that not only does the number of niches not converge given changes to the $\phi$ term, but also that accuracy remains relatively consistent even with the reduction to only five bit-strings per feature set.\\
\begin{figure}[t]
\centering
\fbox{\rule{0pt}{2in}
  \includegraphics[width=0.45\textwidth]{graph3.png}}
\caption{The exploration/exploitation term over several iterations of the control loop.}
\label{fig:graph3}
\end{figure}
\begin{figure}[t]
\centering
\fbox{\rule{0pt}{2in}
  \includegraphics[width=0.45\textwidth]{graph4.png}}
\caption{The accuracy over several iterations of the control loop.}
\label{fig:graph3}
\end{figure}
\indent Figure 5 and Figure 6 show the results of our experiments with the controller gains. The experiments were run with the set point, desired number of clusters, as 4. Figure 5 shows the classification accuracy for each iteration of the control loop. Figure 6 shows the corresponding $\phi$ term used for the genetic algorithm based on the number of niches returned in the previous iteration. The parameters for each trial were:

\begin{center}
\begin{tabular}{ l | l | l | l }
  Trial & P & I & D \\ \hline
  $0$ & $0.1$ & $0$ & $0$  \\
  $1$ &   $1$ & $0$ & $0$ \\
  $2$ &   $1$ & $0$ & $0.1$ \\
  $3$ &   $1$ & $0.001$ & $0.1$  \\
  $4$ &  $10$ & $0$ & $0.5$ 
\end{tabular} \\
\end{center}


The results shows that the classification accuracy is invariant of the controller. Coupled with the results in Figure 4, this graph also shows that the $\phi$ term does not have a significant impact on the convergence in the number of niches. Figure 6 show $\phi$ term has a constant increase. This consistency is due to the fact that the error from each iteration controls the change in $\phi$ for the next iteration. Since the error is consistent, the change in $\phi$ is consistent.\\
\indent Based on all the results shown above, we conclude that the controller had minimal impact on the genetic algorithm. The unchanging results of the clustering algorithm show that, while our results do not show convergence, we cannot conclude that the controller does not have an impact on the efficiency of the algorithm. The results suggest that further experimentation with additional optimization algorithms is need to fully test the possibilities of a feedback controller. In addition, the results shows that with given fitness function for clustering, we may not be guaranteed to converge to desired number of clusters or optimal solutions. 

\section{Conclusion and Future Work}
This paper details the use of genetic algorithm, clustering, and feedback control in order to fixed-size optimized feature sets for classification. A PID controller adjusts the parameters of the genetic algorithm in order to control the range of the solution space. Clustering groups the solution space into a set of niches. This set of niches determines the error for the control loop in the next iteration of the feedback loop. This paper explores the use of derivative and integral terms in order to increase the responsiveness of the controller. In addition, it explore the use of different distance metrics used for clustering in order to investigate the characteristics of each of the solution space's niches. Although the results did not yield great success in controlling the solution space, the work detailed leaves several areas for further investigation and experimentation.\\
\indent For future work, we would experiment with other distance metrics for clustering. As mentioned in the results section, the classification results proved to be similar to create substantial differences in clustering. This similarity caused the solution set to sometimes converge to a single cluster, which impede the feedback controller.\\
\indent Another area for future work would be the creation of classification rules in the genetic algorithm. In our current algorithm, we relies solely on Naive Bayes as the classifier for a given feature. By expanding upon Naive Bayes, we could implement a decision tree based on the Naive Bayes results for each element of a given feature set. This expansion should improve the classification scores of the feature set as it considers combinations of words in a title rather just each word by itself.

\begin{thebibliography}{100}
	
	\bibitem{ole:feedback} Shi, J., Mengshoel, O.J., and Pal,D.K., "Feedback Control for Multi-modal Optimization using Genetic Algorithms," \emph{Carnegie Mellon University}.
	\bibitem{feature:feedback} Vafaie, H. and Jong, K.D., "Genetic Algorithms as a Tool for Feature Selection in Machine Learning," \emph{Center for Artificial Intelligence, George Mason University}.
	\bibitem{galan:feedback} Galan, S.F., and Mengshoel, O.J, "Generalized Crowding for Genetic Algorithms," \emph{Carnegie Mellon University}.
	\bibitem{boydstun}The University of Texas at Austin. (2006). "Boydstun NYT Front Page Dataset". <http://www.policyagendas.org/document/boydstun-nyt-front-page-dataset>
	
\end{thebibliography}

\end{document}
